{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30408,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/latinchakma/clip-for-image-text-matching-bangla?scriptVersionId=172011027\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport cv2\nimport gc\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom tqdm.autonotebook import tqdm\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport timm\nfrom transformers import DistilBertTokenizer, DistilBertModel\nos.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:18.234902Z","iopub.execute_input":"2023-04-21T17:35:18.235476Z","iopub.status.idle":"2023-04-21T17:35:32.442275Z","shell.execute_reply.started":"2023-04-21T17:35:18.235418Z","shell.execute_reply":"2023-04-21T17:35:32.441178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/flickr-image-dataset/flickr30k_images/results.csv\", delimiter=\"|\")\ndf.columns = ['image', 'caption_number', 'caption']\ndf['caption'] = df['caption'].str.lstrip()\ndf['caption_number'] = df['caption_number'].str.lstrip()\ndf.loc[19999, 'caption_number'] = \"4\"\ndf.loc[19999, 'caption'] = \"A dog runs across the grass .\"\nids = [id_ for id_ in range(len(df) // 5) for i in range(5)]\ndf['id'] = ids\ndf.to_csv(\"captions.csv\", index=False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:32.444573Z","iopub.execute_input":"2023-04-21T17:35:32.445453Z","iopub.status.idle":"2023-04-21T17:35:33.495962Z","shell.execute_reply.started":"2023-04-21T17:35:32.445385Z","shell.execute_reply":"2023-04-21T17:35:33.494909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:33.497638Z","iopub.execute_input":"2023-04-21T17:35:33.497996Z","iopub.status.idle":"2023-04-21T17:35:45.921175Z","shell.execute_reply.started":"2023-04-21T17:35:33.497959Z","shell.execute_reply":"2023-04-21T17:35:45.920136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:45.927959Z","iopub.execute_input":"2023-04-21T17:35:45.928299Z","iopub.status.idle":"2023-04-21T17:35:46.414349Z","shell.execute_reply.started":"2023-04-21T17:35:45.928264Z","shell.execute_reply":"2023-04-21T17:35:46.413339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nnow = datetime.now()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:46.419231Z","iopub.execute_input":"2023-04-21T17:35:46.421566Z","iopub.status.idle":"2023-04-21T17:35:46.427638Z","shell.execute_reply.started":"2023-04-21T17:35:46.421527Z","shell.execute_reply":"2023-04-21T17:35:46.42676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:46.432211Z","iopub.execute_input":"2023-04-21T17:35:46.435347Z","iopub.status.idle":"2023-04-21T17:35:46.441711Z","shell.execute_reply.started":"2023-04-21T17:35:46.43531Z","shell.execute_reply":"2023-04-21T17:35:46.440742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(inspect.getsource(SentenceTransformer))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:46.445207Z","iopub.execute_input":"2023-04-21T17:35:46.446664Z","iopub.status.idle":"2023-04-21T17:35:46.452863Z","shell.execute_reply.started":"2023-04-21T17:35:46.446627Z","shell.execute_reply":"2023-04-21T17:35:46.451482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    debug = False\n    image_path = \"../input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\n    captions_path = \".\"\n    batch_size = 50\n    num_workers = 2\n    head_lr = 1e-3\n    image_encoder_lr = 1e-4\n    text_encoder_lr = 1e-5\n    weight_decay = 1e-3\n    patience = 1\n    factor = 0.8\n    epochs = 2\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model_name = 'resnet50'\n    image_embedding = 2048\n    text_encoder_model = 'paraphrase-multilingual-mpnet-base-v2'\n    text_embedding = 768\n    text_tokenizer = \"distilbert-base-uncased\"\n    max_length = 200\n\n    pretrained = True \n    trainable = True \n    temperature = 1.0\n\n    \n    size = 224\n\n    ; used for both image and text encoders\n    num_projection_layers = 1\n    projection_dim = 256 \n    dropout = 0.1","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:46.455495Z","iopub.execute_input":"2023-04-21T17:35:46.45749Z","iopub.status.idle":"2023-04-21T17:35:46.54159Z","shell.execute_reply.started":"2023-04-21T17:35:46.456936Z","shell.execute_reply":"2023-04-21T17:35:46.540485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n\n    def reset(self):\n        self.avg, self.sum, self.count = [0] * 3\n\n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum / self.count\n\n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:46.543285Z","iopub.execute_input":"2023-04-21T17:35:46.543654Z","iopub.status.idle":"2023-04-21T17:35:46.554213Z","shell.execute_reply.started":"2023-04-21T17:35:46.543614Z","shell.execute_reply":"2023-04-21T17:35:46.553218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getImageFeatures(image_filename):\n    image = cv2.imread(f\"{CFG.image_path}/{image_filename}\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = get_transforms()(image=image)['image']\n    image = torch.tensor(image).permute(2, 0, 1).float()\n    return image\ndef getTextEncodings(caption,tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)):\n    encoded_captions=tokenizer(\n        list([caption]), padding=True, truncate=True, max_length=CFG.max_length\n    )\n    return encoded_captions\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:46.559618Z","iopub.execute_input":"2023-04-21T17:35:46.559897Z","iopub.status.idle":"2023-04-21T17:35:47.981447Z","shell.execute_reply.started":"2023-04-21T17:35:46.55987Z","shell.execute_reply":"2023-04-21T17:35:47.980378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIPDataset(torch.utils.data.Dataset):\n    def __init__(self, image_filenames, captions, tokenizer, transforms):\n\n\n        self.image_filenames = image_filenames\n        self.captions = list(captions)\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        item = {}\n        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.transforms(image=image)['image']\n        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n        item['caption'] = self.captions[idx]\n\n        return item\n\n\n    def __len__(self):\n        return len(self.captions)\n\n\n\ndef get_transforms(mode=\"train\"):\n    if mode == \"train\":\n        return A.Compose(\n            [\n                A.Resize(CFG.size, CFG.size, always_apply=True),\n                A.Normalize(max_pixel_value=255.0, always_apply=True),\n            ]\n        )\n    else:\n        return A.Compose(\n            [\n                A.Resize(CFG.size, CFG.size, always_apply=True),\n                A.Normalize(max_pixel_value=255.0, always_apply=True),\n            ]\n        )","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:47.983023Z","iopub.execute_input":"2023-04-21T17:35:47.983635Z","iopub.status.idle":"2023-04-21T17:35:47.995576Z","shell.execute_reply.started":"2023-04-21T17:35:47.98359Z","shell.execute_reply":"2023-04-21T17:35:47.994487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    \"\"\"\n    Encode images to a fixed size vector\n    \"\"\"\n\n    def __init__(\n        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n    ):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n        )\n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:47.997315Z","iopub.execute_input":"2023-04-21T17:35:47.997717Z","iopub.status.idle":"2023-04-21T17:35:48.008566Z","shell.execute_reply.started":"2023-04-21T17:35:47.997671Z","shell.execute_reply":"2023-04-21T17:35:48.007521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n        super().__init__()\n        self.model = SentenceTransformer(model_name)\n        for p in self.model.parameters():\n            p.requires_grad = trainable\n        self.target_token_idx = 0\n\n    def forward(self, caption):\n        output = self.model.encode(caption,show_progress_bar=False)\n        output_tensor = torch.from_numpy(output)\n        return output_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:48.009833Z","iopub.execute_input":"2023-04-21T17:35:48.01106Z","iopub.status.idle":"2023-04-21T17:35:48.019769Z","shell.execute_reply.started":"2023-04-21T17:35:48.01102Z","shell.execute_reply":"2023-04-21T17:35:48.018626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProjectionHead(nn.Module):\n    def __init__(\n        self,\n        embedding_dim,\n        projection_dim=CFG.projection_dim,\n        dropout=CFG.dropout\n    ):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n    \n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x = x + projected\n        x = self.layer_norm(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:48.021553Z","iopub.execute_input":"2023-04-21T17:35:48.021961Z","iopub.status.idle":"2023-04-21T17:35:48.034634Z","shell.execute_reply.started":"2023-04-21T17:35:48.021925Z","shell.execute_reply":"2023-04-21T17:35:48.033623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIPModel(nn.Module):\n    def __init__(\n        self,\n        temperature=CFG.temperature,\n        image_embedding=CFG.image_embedding,\n        text_embedding=CFG.text_embedding,\n    ):\n        super().__init__()\n        self.image_encoder = ImageEncoder()\n        self.text_encoder = TextEncoder()\n        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n        self.temperature = temperature\n\n    def forward(self, batch):\n        # Getting Image and Text Features\n        image_features = self.image_encoder(batch[\"image\"])\n        text_features = self.text_encoder(batch[\"caption\"]).to(CFG.device)\n        # Getting Image and Text Embeddings (with same dimension)\n        image_embeddings = self.image_projection(image_features)\n        text_embeddings = self.text_projection(text_features)\n\n        # Calculating the Loss\n        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n        logits = F.softmax(logits, dim=-1)\n#         images_similarity = image_embeddings @ image_embeddings.T\n#         texts_similarity = text_embeddings @ text_embeddings.T\n#         targets = F.softmax(\n#             (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n#         )\n#         texts_loss = cross_entropy(logits, targets, reduction='none')\n#         images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n#         loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n        target=torch.tensor(np.arange(0,len(batch['image']))).to(CFG.device)\n        loss = nn.CrossEntropyLoss()(logits,target)\n        return loss.mean()\n\n\ndef cross_entropy(preds, targets, reduction='none'):\n    log_softmax = nn.LogSoftmax(dim=-1)\n    loss = (-targets * log_softmax(preds)).sum(1)\n    if reduction == \"none\":\n        return loss\n    elif reduction == \"mean\":\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:48.036447Z","iopub.execute_input":"2023-04-21T17:35:48.036856Z","iopub.status.idle":"2023-04-21T17:35:48.049265Z","shell.execute_reply.started":"2023-04-21T17:35:48.03682Z","shell.execute_reply":"2023-04-21T17:35:48.048054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_train_valid_dfs():\n    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n    image_ids = np.arange(0, max_id)\n    np.random.seed(42)\n    valid_ids = np.random.choice(\n        image_ids, size=int(0.2 * len(image_ids)), replace=False\n    )\n    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n    return train_dataframe, valid_dataframe\n\n\ndef build_loaders(dataframe, tokenizer, mode):\n    transforms = get_transforms(mode=mode)\n    dataset = CLIPDataset(\n        dataframe[\"image\"].values,\n        dataframe[\"caption\"].values,\n        tokenizer=tokenizer,\n        transforms=transforms,\n    )\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=CFG.batch_size,\n        num_workers=CFG.num_workers,\n        shuffle=True if mode == \"train\" else False\n    )\n    return dataloader\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:48.050938Z","iopub.execute_input":"2023-04-21T17:35:48.051364Z","iopub.status.idle":"2023-04-21T17:35:48.065415Z","shell.execute_reply.started":"2023-04-21T17:35:48.051328Z","shell.execute_reply":"2023-04-21T17:35:48.064283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = CLIPModel().to(CFG.device)\nmodel = CLIPModel().to(CFG.device)\n# model.load_state_dict(torch.load('best.pt', map_location=CFG.device))\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:35:48.067083Z","iopub.execute_input":"2023-04-21T17:35:48.067699Z","iopub.status.idle":"2023-04-21T17:36:12.530459Z","shell.execute_reply.started":"2023-04-21T17:35:48.06766Z","shell.execute_reply":"2023-04-21T17:36:12.529384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nnow = datetime.now()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:36:12.53207Z","iopub.execute_input":"2023-04-21T17:36:12.532451Z","iopub.status.idle":"2023-04-21T17:36:12.539456Z","shell.execute_reply.started":"2023-04-21T17:36:12.532397Z","shell.execute_reply":"2023-04-21T17:36:12.538298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(img_name,caption):\n    img=getImageFeatures(img_name)\n    data=dict()\n    data['image']=torch.tensor(np.array(img)).to(CFG.device)\n    data['image']=torch.reshape(data['image'],(1,3,224,224))\n    \n    x=model.image_encoder(data['image'])\n    y=model.text_encoder(caption).to(CFG.device)\n    x=model.image_projection(x)\n    y=model.text_projection(y)\n    x = F.normalize(x, p=2, dim=-1)\n    y = F.normalize(y, p=2, dim=-1)\n#     output = nn.CosineSimilarity()(x,y)\n#     print(output)\n    output = x @ y.T\n    return output","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:36:12.541102Z","iopub.execute_input":"2023-04-21T17:36:12.541518Z","iopub.status.idle":"2023-04-21T17:36:12.56454Z","shell.execute_reply.started":"2023-04-21T17:36:12.541479Z","shell.execute_reply":"2023-04-21T17:36:12.563602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n    loss_meter = AvgMeter()\n    tqdm_object = tqdm(train_loader, total=len(train_loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) if k!=\"caption\" else v for k, v in batch.items()}\n        loss = model(batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if step == \"batch\":\n            lr_scheduler.step()\n\n        count = batch[\"image\"].size(0)\n        loss_meter.update(loss.item(), count)\n\n        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n    return loss_meter\n\n\ndef valid_epoch(model, valid_loader):\n    loss_meter = AvgMeter()\n\n    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) if k!=\"caption\" else v for k, v in batch.items()}\n        loss = model(batch)\n\n        count = batch[\"image\"].size(0)\n        loss_meter.update(loss.item(), count)\n\n        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n    return loss_meter\n\n\ndef main():\n    train_df, valid_df = make_train_valid_dfs()\n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n\n\n    \n    params = [\n        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n        {\"params\": itertools.chain(\n            model.image_projection.parameters(), model.text_projection.parameters()\n        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n    ]\n    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n    )\n    step = \"epoch\"\n\n    best_loss = float('inf')\n    for epoch in range(CFG.epochs):\n        print(f\"Epoch: {epoch + 1}\")\n        model.train()\n        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n        model.eval()\n        with torch.no_grad():\n            print(inference(\"1033767085.jpg\",\"Two children are having breakfast\"))\n            print(inference(\"1033767085.jpg\",\"An old man riding a cycle on top a brdige\"))\n            print(inference(\"1033767085.jpg\",\"দুটি বাচ্চা নাস্তা করছে\"))\n            valid_loss = valid_epoch(model, valid_loader)\n            current_time = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\");\n            torch.save(model.state_dict(),current_time+'.pt')\n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            torch.save(model.state_dict(), \"bestest.pt\")\n            print(\"Saved Best Model!\")\n        \n        lr_scheduler.step(valid_loss.avg)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:36:12.566322Z","iopub.execute_input":"2023-04-21T17:36:12.566698Z","iopub.status.idle":"2023-04-21T17:36:12.584755Z","shell.execute_reply.started":"2023-04-21T17:36:12.566663Z","shell.execute_reply":"2023-04-21T17:36:12.583641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:36:12.586078Z","iopub.execute_input":"2023-04-21T17:36:12.586981Z","iopub.status.idle":"2023-04-21T17:48:34.97394Z","shell.execute_reply.started":"2023-04-21T17:36:12.586942Z","shell.execute_reply":"2023-04-21T17:48:34.971805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(),'best1.pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:48:34.975674Z","iopub.status.idle":"2023-04-21T17:48:34.976577Z","shell.execute_reply.started":"2023-04-21T17:48:34.976278Z","shell.execute_reply":"2023-04-21T17:48:34.976309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(valid_df, model_path):\n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n    \n    model = CLIPModel().to(CFG.device)\n    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n    model.eval()\n    \n    valid_image_embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(valid_loader):\n            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n            image_embeddings = model.image_projection(image_features)\n            valid_image_embeddings.append(image_embeddings)\n    return model, torch.cat(valid_image_embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:48:34.978647Z","iopub.status.idle":"2023-04-21T17:48:34.979163Z","shell.execute_reply.started":"2023-04-21T17:48:34.978898Z","shell.execute_reply":"2023-04-21T17:48:34.978925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_matches(model, image_embeddings, query, image_filenames, n=9):\n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n    encoded_query = tokenizer([query])\n    batch = {\n        key: torch.tensor(values).to(CFG.device)\n        for key, values in encoded_query.items()\n    }\n    with torch.no_grad():\n        text_features = model.text_encoder(\n            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n        )\n        text_embeddings = model.text_projection(text_features)\n    \n    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n    \n    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n    matches = [image_filenames[idx] for idx in indices[::5]]\n    \n    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n    for match, ax in zip(matches, axes.flatten()):\n        image = cv2.imread(f\"{CFG.image_path}/{match}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        ax.imshow(image)\n        ax.axis(\"off\")\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:48:34.980864Z","iopub.status.idle":"2023-04-21T17:48:34.98287Z","shell.execute_reply.started":"2023-04-21T17:48:34.982575Z","shell.execute_reply":"2023-04-21T17:48:34.982606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.load_state_dict(torch.load('/kaggle/working/04-21-2023-16-21-02.pt',map_location=CFG.device))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:48:34.984552Z","iopub.status.idle":"2023-04-21T17:48:34.985444Z","shell.execute_reply.started":"2023-04-21T17:48:34.985141Z","shell.execute_reply":"2023-04-21T17:48:34.98517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:48:34.987047Z","iopub.status.idle":"2023-04-21T17:48:34.987921Z","shell.execute_reply.started":"2023-04-21T17:48:34.98765Z","shell.execute_reply":"2023-04-21T17:48:34.987678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(inference(\"1034276567.jpg\",\"A kid is eating\"))\nprint(inference(\"1034276567.jpg\",\"একটি ছেলে খাবার খাচ্ছে\"))\nprint(inference(\"1034276567.jpg\",\"দুটি কুকুর খেলা করছে সমুদ্রের সৈকতে\"))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:48:34.989558Z","iopub.status.idle":"2023-04-21T17:48:34.990482Z","shell.execute_reply.started":"2023-04-21T17:48:34.990117Z","shell.execute_reply":"2023-04-21T17:48:34.990145Z"},"trusted":true},"execution_count":null,"outputs":[]}]}